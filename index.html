<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Scroll-Triggered Audio (BufferSource)</title>
  <style>
    html, body { margin:0; padding:0; }
    #startStop {
      position: fixed; top:10px; left:10px; z-index:100;
      padding:8px 12px; font-size:16px;
    }
    section {
      height:100vh;
      display:flex; justify-content:center; align-items:center;
      font-size:2rem;
    }
    section[data-track="rainAudio"]  { background:#e0f7fa; }
    section[data-track="pianoAudio"] { background:#ffe0b2; }
    section[data-track="kidsAudio"]  { background:#c8e6c9; }
  </style>
</head>
<body>
  <button id="startStop">Start Audio</button>

  <section data-track="rainAudio">Rain Sound</section>
  <section data-track="pianoAudio">Piano Sound</section>
  <section data-track="kidsAudio">Kids Playing</section>

  <script>
    let audioCtx = null;
    const buffers = {};   // will hold decoded AudioBuffers
    const sources = {};   // will hold active BufferSourceNodes

    document.getElementById('startStop')
      .addEventListener('click', async function toggleAudio() {
      const btn = this;
      // If not started, initialize and decode
      if (!audioCtx) {
        btn.textContent = 'Loadingâ€¦';
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume();

        // 1) Fetch & decode all tracks
        const tracks = [
          { id: 'rainAudio',  url: 'rain.mp3' },
          { id: 'pianoAudio', url: 'piano.mp3' },
          { id: 'kidsAudio',  url: 'kids_playing.mp3' }
        ];
        await Promise.all(tracks.map(async t => {
          const res = await fetch(t.url);
          const ab  = await res.arrayBuffer();
          buffers[t.id] = await audioCtx.decodeAudioData(ab);
        }));

        btn.textContent = 'Start Audio';

        // 2) Observe sections; fire when fully in view
        const observer = new IntersectionObserver((entries) => {
          entries.forEach(entry => {
            if (entry.intersectionRatio === 1) {
              const id = entry.target.dataset.track;
              if (!sources[id]) {
                // Create source + gain for this track
                const src  = audioCtx.createBufferSource();
                const gain = audioCtx.createGain();
                src.buffer   = buffers[id];
                src.loop     = true;
                gain.gain.value = 0;
                src.connect(gain).connect(audioCtx.destination);
                src.start();
                // fade in
                gain.gain.linearRampToValueAtTime(0.5, audioCtx.currentTime + 0.1);
                sources[id] = src;
              }
            }
          });
        }, { threshold: 1.0 });

        document.querySelectorAll('section')
          .forEach(sec => observer.observe(sec));

        btn.textContent = 'Stop Audio';
      }
      else {
        // Stop & clean up
        Object.values(sources).forEach(src => src.stop());
        await audioCtx.close();
        audioCtx = null;
        Object.keys(buffers).forEach(k => delete buffers[k]);
        Object.keys(sources).forEach(k => delete sources[k]);
        btn.textContent = 'Start Audio';
      }
    });
  </script>
</body>
</html>
